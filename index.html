<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Coupling Vision and Proprioception for Navigation of Legged Robots">
  <meta name="keywords" content="Locomotion, Biomechanics, Energetics, Reinforcement Learning, Emergent Behaviors">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Coupling Vision and Proprioception for Navigation of Legged Robots</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <link rel="icon" href="./favicon.ico?">

  <meta property="og:site_name" content="Coupling Vision and Proprioception for Navigation of Legged Robots" />
  <meta property="og:type" content="video.other" />
  <meta property="og:title" content="Coupling Vision and Proprioception for Navigation of Legged Robots" />
  <meta property="og:url" content="https://navigation-locomotion.github.io/camera-ready/" />
  <meta property="og:image" content="https://navigation-locomotion.github.io/camera-ready/static/images/preview.jpg" />
  <meta property="og:image:secure" content="https://navigation-locomotion.github.io/camera-ready/static/images/preview.jpg" />
  <meta property="og:video" content="https://www.youtube.com/embed/sZVvutQUAQ4" />
  <meta property="og:video:secure" content="https://www.youtube.com/embed/sZVvutQUAQ4" />

  <meta property="article:publisher" content="https://markfzp.github.io" />
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Coupling Vision and Proprioception for Navigation of Legged Robots" />
  <meta name="twitter:url" content="https://navigation-locomotion.github.io/camera-ready/" />
  <meta name="twitter:image" content="https://navigation-locomotion.github.io/camera-ready/static/images/preview.jpg" />
  <meta property="og:image:width" content="1600" />
  <meta property="og:image:height" content="900" />

  <script src="https://www.youtube.com/iframe_api"></script>
  <meta name="twitter:card" content="player" />
  <meta name="twitter:image" content="https://navigation-locomotion.github.io/camera-ready/static/images/preview.jpg" />
  <meta name="twitter:player" content="https://www.youtube.com/embed/sZVvutQUAQ4" />
  <meta name="twitter:player:width" content="640" />
  <meta name="twitter:player:height" content="360" />
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Coupling Vision and Proprioception for <br/> Navigation of Legged Robots</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://markfzp.github.io/">Zipeng Fu</a><sup>*</sup><sup>1</sup>&nbsp;&nbsp;&nbsp;
              <a href="https://ashish-kmr.github.io/">â€ªAshish Kumar</a><sup>*</sup><sup>2</sup>&nbsp;&nbsp;&nbsp;
              <a href="https://anag.me/">Ananye Agarwal</a><sup>1</sup>&nbsp;&nbsp;&nbsp;
              <a href="https://haozhi.io/">Haozhi Qi</a><sup>2</sup>&nbsp;&nbsp;&nbsp;
              <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a><sup>2</sup>&nbsp;&nbsp;&nbsp;
              <a href="https://www.cs.cmu.edu/~dpathak/">Deepak Pathak</a><sup>1</sup>
              <br /><sup>1</sup>Carnegie Mellon University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>2</sup>UC Berkeley
              <span class="brmod"><b>CVPR 2022</b></span>
              <span class="brmod">(<a href="https://mula-workshop.github.io/"><b>Best Paper Award at Multimodal Learning Workshop</b>`</a>)</span>
            </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="./resources/navigation-locomotion.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- arXiv Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2112.02094"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="#method_video"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/MarkFzp/navigation-locomotion" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com -->
                  </span>
                  <span>Code</span>
                </a>
              </span>

            </div>

          </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="hero-body">
    <div class="columns is-centered">
      <div class="column is-9">
        <div class="is-centered">
          <video autoplay muted loop playsinline style="width: 45%; border: 1px solid #bbb; border-radius: 10px; margin: 2.0%;">
            <source src="./resources/glass_door_comparison_teaser.mp4" type="video/mp4">
          </video>
          <video autoplay muted loop playsinline style="width: 45%; border: 1px solid #bbb; border-radius: 10px; margin: 2.0%;">
            <source src="./resources/human_obstacle_comparison_teaser.mp4" type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            <strong>Coupling vision and proprioception helps navigation in situations where vision alone is not enough</strong>
          </h2>
      </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container">

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
        <h2 class="title is-2">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We exploit the complementary strengths of vision and proprioception to achieve point goal navigation in a legged robot. Legged systems are capable of traversing more complex terrain than wheeled robots, but to fully exploit this capability, we need the high-level path planner in the navigation system to be aware of the walking capabilities of the low-level locomotion policy on varying terrains. We achieve this by using proprioceptive feedback to estimate the safe operating limits of the walking policy, and to sense unexpected obstacles and terrain properties like smoothness or softness of the ground that may be missed by vision. The navigation system uses onboard cameras to generate an occupancy map and a corresponding cost map to reach the goal. The FMM (Fast Marching Method) planner then generates a target path. The velocity command generator takes this as input to generate the desired velocity for the locomotion policy using as input additional constraints, from the safety advisor, of unexpected obstacles and terrain determined speed limits. We show superior performance compared to wheeled robot (LoCoBot) baselines, and other baselines which have disjoint high-level planning and low-level control. We also show the real-world deployment of our system on a quadruped robot with onboard sensors and compute. 
          </p>
        </div>
      </div>
    </div>
  </div>

  <!--/ Paper video. -->
  <br/>
  <br/>
  <div id="method_video" class="columns is-centered has-text-centered">
    <div class="column is-two-thirds">
      <h2 class="title is-2">Project Video</h2>
      <div class="publication-video">
        <iframe src="https://www.youtube.com/embed/sZVvutQUAQ4"
                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-2" style="text-align: center;">Navigation in Cluttered Indoor Settings</h2>
        <tr>
          <td>
            <br>
            <div class="columns is-centered has-text-centered">
              <video controls src="./resources/indoor_longrange_converted.mp4" style="border: 1px solid #bbb; border-radius: 10px; width: 80%; margin-bottom: 50px;">
            </div>
          </td>
        </tr>
        <tr>
          <td>
              <h2 class="subtitle has-text-centered">
                  The robot can navigate even with glass doors, rough terrain and extra 2kg of payload added during walking. Propriopcetion enables slowing down for safety.
              </h2>
          </td>
        </tr>
        <tr>
          <td>
            <br>
            <div class="columns is-centered has-text-centered">
              <video controls src="./resources/indoor_cluttered_converted.mp4" style="border: 1px solid #bbb; border-radius: 10px; width: 80%; margin-bottom: 50px;">
            </div>
          </td>
        </tr>
        <tr>
          <td>
              <h2 class="subtitle has-text-centered">
                  The robot can navigate to exit a cluttered lab environment full of objects of different shapes, textures and lightning conditions and can recover from collision with the unexpected obstacles using proprioception. 
              </h2>
          </td>
        </tr>
      </div> 
    </div>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-2" style="text-align: center;">Outdoor Navigation</h2>
        <tr>
          <td>
            <br>
            <div class="columns is-centered has-text-centered">
              <video controls src="./resources/inthewild_converted.mp4" style="border: 1px solid #bbb; border-radius: 10px; width: 80%; margin-bottom: 50px;">
            </div>
          </td>
        </tr>
        <tr>
          <td>
              <h2 class="subtitle has-text-centered">
                  Navigation in the wild with a goal of entering a building 30m away. The robot needs to go through water puddles and lawns. To furthur increase the difficulty, a 2kg mass is thrown at the robot and an unexpected human obstacle blindsided it. The robot can overcome these by coupling vision and proprioception.   
              </h2>
          </td>
        </tr>
      </div> 
    </div>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-2" style="text-align: center;">Collision Detector</h2>
        <tr>
          <td>
            <br>
            <div class="columns is-centered has-text-centered">
              <video controls src="./resources/collision_detector_converted.mp4" style="border: 1px solid #bbb; border-radius: 10px; width: 80%; margin-bottom: 50px;">
            </div>
          </td>
        </tr>
        <tr>
          <td>
              <div class="columns is-centered has-text-centered">
                <img src="./resources/collision_detector.png" style="width: 80%; margin-bottom: 50px;">
              </div>
              <h2 class="subtitle has-text-centered">
                  The robot collides with a glass door (invisible to the onboard camera), detects the collision using proprioception and then walks around it to reach the goal. The vision-only baseline fails. 
              </h2>
          </td>
        </tr>
      </div> 
    </div>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-2" style="text-align: center;">Vision + Proprioception vs. Only Vision</h2>
        <tr>
          <td>
            <br>
            <div class="columns is-centered has-text-centered">
              <video controls src="./resources/vision_v_prop_night_converted.mp4" style="border: 1px solid #bbb; border-radius: 10px; width: 80%; margin-bottom: 20px;">
            </div>
          </td>
        </tr>
        <tr>
          <td>
              <h2 class="subtitle has-text-centered" style="margin-bottom: 50px;">
                  Avoiding a tree using proprioception. In low-lighting conditions, the depth camera is unable to detect the tree.
              </h2>
          </td>
        </tr>
        <tr>
          <td>
            <br>
            <div class="columns is-centered has-text-centered">
              <video controls src="./resources/vision_v_prop_human_converted.mp4" style="border: 1px solid #bbb; border-radius: 10px; width: 80%; margin-bottom: 20px;">
            </div>
          </td>
        </tr>
        <tr>
          <td>
              <h2 class="subtitle has-text-centered" style="margin-bottom: 50px;">
                  A human obstacle suddenly appearing from outside of FoV. The robot relies on proprioception to detect the collision and navigate around it. 
              </h2>
          </td>
        </tr>
        <tr>
          <td>
            <br>
            <div class="columns is-centered has-text-centered">
              <video controls src="./resources/vision_v_prop_glass_converted.mp4" style="border: 1px solid #bbb; border-radius: 10px; width: 80%; margin-bottom: 20px;">
            </div>
          </td>
        </tr>
        <tr>
          <td>
              <h2 class="subtitle has-text-centered" style="margin-bottom: 50px;">
                  A glass obstacle is invisible to the depth camera. By using proprioception, the robot can recover from the collision and successfully reach the goal. 
              </h2>
          </td>
        </tr>
      </div> 
    </div>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-2" style="text-align: center;">Fall Predictor</h2>
        <tr>
          <td>
            <br>
            <div class="columns is-centered has-text-centered">
              <video controls src="./resources/fall_predictor_converted.mp4" style="border: 1px solid #bbb; border-radius: 10px; width: 80%; margin-bottom: 50px;">
            </div>
          </td>
        </tr>
        <tr>
          <td>
              <div class="columns is-centered has-text-centered">
                <img src="./resources/fall_predictor.png" style="width: 80%; margin-bottom: 50px;">
              </div>
              <h2 class="subtitle has-text-centered">
                  The fall predictor predicts a high probability of falling down when the robot is walking on unstable planks and decreases the command velocity to safer values. 
              </h2>
          </td>
        </tr>
      </div> 
    </div>
  </div>
</section>

<section class="section">
  <div class="columns is-centered">
    <div class="column is-full-width">
      <div class="is-vcentered interpolation-panel">
        <h2 class="title is-2" style="text-align: center;">Related Work</h2>
        <div class="content is-centered has-text-centered">
          This work is part of our series of research on learning based control of legged robots. Please check out other works below. 
        </div>
        <br><br>
        <div class="container">
          <div class="subtitle is-centered">
            <table width="100%">
              <tr>
                <td width="100%">
                <div style="float:left;margin-right:30px;margin-left:30px;">
                <video autoplay loop muted src="resources/rma-clip.mp4" width="400"></video>
                </div>
                RMA: Rapid Motor Adaptation for Legged Robots <br/>
                Ashish Kumar, Zipeng Fu, Deepak Pathak, Jitendra Malik <br/>
                RSS 2021 <br/> <br/> <br/> 
                <a href="https://ashish-kmr.github.io/rma-legged-robots/rma-locomotion-final.pdf">PDF</a> | 
                <a href="https://www.youtube.com/watch?v=nBy1piJrq1A">Video</a> |
                <a href="https://ashish-kmr.github.io/rma-legged-robots/">Project Page</a> 
              </tr>
            </table>
          </div>
          <div class="subtitle is-centered">
            <table width="100%">
              <tr>
                <td width="100%">
                  <div style="float:left;margin-right:30px;margin-left:30px;">
                  <video autoplay loop muted src="resources/gait-clip.mp4" width="400"></video>
                  </div>
                  Minimizing Energy Consumption Leads to the Emergence of Gaits in Legged Robots <br/>
                  Zipeng Fu, Ashish Kumar, Jitendra Malik, Deepak Pathak <br/>
                  CoRL 2021 <br/> <br/> <br/>
                  <a href="https://arxiv.org/abs/2111.01674">PDF</a> | 
                  <a href="https://www.youtube.com/watch?v=OQN5W2IAb9k">Video</a> |
                  <a href="https://energy-locomotion.github.io/">Project Page</a> 
              </tr>
            </table>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container content">
    <h2 class="titile">BibTeX</h2>
    <pre><code>@misc{fu2021coupling,
      title={Coupling Vision and Proprioception for Navigation of Legged Robots}, 
      author={Zipeng Fu and Ashish Kumar and Ananye Agarwal and Haozhi Qi and Jitendra Malik and Deepak Pathak},
      year={2021},
      eprint={2112.02094},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>Page template borrowed from <a href="https://worldsheet.github.io/"><span class="dnerf">WorldSheet</span></a> and <a href="https://energy-locomotion.github.io/"><span class="dnerf">Energy-Locomotion</span></a>.</p>
    </div>
  </div>
</footer>

</body>
</html>
